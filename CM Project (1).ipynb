{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a92da74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Twitter HashTag to search for\n",
      "China\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "def scrape(words, numtweet):        \n",
    "        tweets = tweepy.Cursor(api.search_tweets,\n",
    "                               words, lang=\"en\",\n",
    "                               #since_id=date_since,\n",
    "                               tweet_mode='extended').items(numtweet)\n",
    "        list_tweets = [tweet for tweet in tweets]\n",
    "        i = 1\n",
    "        tweets=[] \n",
    "        for tweet in list_tweets:\n",
    "                username = tweet.user.screen_name\n",
    "                description = tweet.user.description\n",
    "                location = tweet.user.location\n",
    "                following = tweet.user.friends_count\n",
    "                followers = tweet.user.followers_count\n",
    "                totaltweets = tweet.user.statuses_count\n",
    "                retweetcount = tweet.retweet_count\n",
    "                hashtags = tweet.entities['hashtags']\n",
    " \n",
    "                try:\n",
    "                        text = tweet.retweeted_status.full_text\n",
    "                except AttributeError:\n",
    "                        text = tweet.full_text\n",
    "                hashtext = list()\n",
    "                tweets.append(text)\n",
    "                 \n",
    "                \n",
    "                i = i+1        \n",
    "        return tweets\n",
    "        \n",
    " \n",
    "\n",
    "consumer_key = \"HbMr5aI8SfdzpW0cRVixlouug\" \n",
    "consumer_secret = \"VvQwaesyBBnVqRbAa9I5p66Gsz60JXkpT9WuDgzW8ZrdOiUt5K\"\n",
    "access_key = \"1197687543061307393-bOV5pyb4aOBqPhhheidthREH5uyP91\"\n",
    "access_secret = \"1o2w5gAbepqvLQpWDkITFFxR6enQ4uRF6cl0DAA3KSuU9\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)\n",
    " \n",
    "      \n",
    "print(\"Enter Twitter HashTag to search for\")\n",
    "words = input()\n",
    "        \n",
    "numtweet = 2000\n",
    "tweets=scrape(words, numtweet)\n",
    "df=pd.DataFrame(tweets, columns=[\"Tweets\"])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a01813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "rev = []\n",
    "for i in tweets:\n",
    "  s = str(i)\n",
    "  x = s.replace(s[:37],\"\").replace(\"<br/><br/>\",\"\").replace(\"</div>\", \"\").replace(\"\\n\", \" \")\n",
    "  rev.append(x.lower())\n",
    "\n",
    "\n",
    "\n",
    "def Punctuation(rev):\n",
    "    punctuation = string.punctuation\n",
    "    lis = []\n",
    "    for sentence in rev:\n",
    "      se=[]\n",
    "      se.clear()\n",
    "      for j in sentence:\n",
    "        #print(j)\n",
    "        if j in punctuation:\n",
    "          se.append(\"\")        \n",
    "        else:\n",
    "          se.append(j)  \n",
    "      lis.append(\"\".join(se).replace(\"i \", \"\"))\n",
    "      \n",
    "       \n",
    "    return lis\n",
    "      \n",
    "def convert_sentence(rev):\n",
    "  reviews=[]\n",
    "  for i in rev:\n",
    "    review.append(i)\n",
    "  return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd6eff54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Tweets': Punctuation(rev)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "831000e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'C:\\Users\\Vivek\\OneDrive\\Desktop\\Recession.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b98fb2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plan of trickle down economics  a policy which historically ends in recession but republicans are also proposing a plan to cut the social security and medicare of every american  a policy which will force retired elderly into poverty'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Tweets\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28591ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Vivek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vivek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vivek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "nltk.download('stopwords')\n",
    "stop=set(stopwords.words('english'))\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def Tokenize(rev):\n",
    "    reviews=[]\n",
    "    for i in rev:\n",
    "        nltk_tokens = nltk.word_tokenize(i)\n",
    "        reviews.append(nltk_tokens)\n",
    "    return reviews\n",
    "\n",
    "def Stemming_Lemmetize(rev):\n",
    "    lem_reviews = []\n",
    "    for i in rev:\n",
    "        len_sen=[]\n",
    "        for j in i:\n",
    "            len_sen.append(ps.stem(lemmatizer.lemmatize(j)))\n",
    "    lem_reviews.append(len_sen)\n",
    "    return lem_reviews\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop=set(stopwords.words('english'))\n",
    "def stop_words(sl):\n",
    "    \n",
    "    stop = set(stopwords.words('english'))\n",
    "    b=[]\n",
    "    for sen in x:\n",
    "        a=[]\n",
    "        for i in sen:\n",
    "            if i in stop or len(i)==1:\n",
    "                continue\n",
    "            else:\n",
    "                a.append(i)\n",
    "    b.append(a)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d58bdef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_43080/1779296150.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#refined_tweets = Stemming_Lemmetize(stop_words(Tokenize(data[\"Tweets\"])))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Tweets\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "#refined_tweets = Stemming_Lemmetize(stop_words(Tokenize(data[\"Tweets\"])))\n",
    "import re\n",
    "x=Tokenize(data[\"Tweets\"])\n",
    "b=[]\n",
    "for sen in x:\n",
    "    a=[]\n",
    "    for i in sen:\n",
    "        if i in stop or len(i)==1 or i[0:5]==\"https\" or i.isdigit() or re.sub(\"\\S*\\d\\S*\", \"\", i)==\"\":\n",
    "            continue\n",
    "        else:\n",
    "            a.append(i)\n",
    "    b.append(a)\n",
    "data['cleaned_tweets'] = {\"Tweets\":b}\n",
    "cleaned_data.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
